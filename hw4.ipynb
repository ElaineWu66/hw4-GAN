{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZfjznO8iZYr"
   },
   "source": [
    "# Homework 4\n",
    "\n",
    "The due date for this assignment is posted on Canvas. There are 22\n",
    "points possible (2 for setup, 20 for your answers in the notebook). This\n",
    "assignment is worth approximately 8% of your final grade.\n",
    "\n",
    "This assignment can be completed in **groups of two**. If you need to find a\n",
    "partner, please post in the pinned note on Piazza. To invite your partner to\n",
    "your group, please go to the \"Settings\" page on GitHub, and click on\n",
    "\"Collaborators and Access\" on the left-hand toolbar. If you are in a repository\n",
    "named hw4-myusername, then this should bring you to a url like\n",
    "\"https://github.com/cs449f23/hw4-myusername/settings/access.\" Then, click\n",
    "\"Add people\" and add your teammate to your repository. Please give them\n",
    "\"Write,\" \"Maintain,\" or \"Admin\" access.\n",
    "\n",
    "Late work is accepted by editing the `LATE_DAYS` file. See `README.md` for more information.\n",
    "\n",
    "## Academic Integrity\n",
    "\n",
    "Once you have formed your groups, the work you submit must belong only to your group members. All group members must contribute to the work submitted; if necessary, we may give group members different grades based on their relative contribution. Do not submit another team's work as your own, and do not allow another team to submit your work as their own.\n",
    "\n",
    "If you use resources you find online, you must cite those in your notebook. If we find sufficiently suspicious similarities between your answers and those of another group, all students involved may be reported for a suspected violation. If you're unsure of the academic integrity policies, ask for help; we can help you avoid breaking the rules, but we can't un-report a suspected violation.\n",
    "\n",
    "By pushing your code to GitHub, you agree to these rules, and understand that there may be severe consequences for violating them.\n",
    "\n",
    "Note: You may reference the LSTM notebooks we provided on Canvas, but you should cite those like any other resource.\n",
    "\n",
    "## Important instructions\n",
    "\n",
    "To submit this assignment, you should commit to your GitHub account:\n",
    "\n",
    "* your Net ID(s) in the netid file; one per line. Please do not put your name in your notebook; we will grade these anonymously.\n",
    "\n",
    "* a hw4.pdf printout of the completed notebook that shows all your answers.\n",
    "\n",
    "* your final hw4.ipynb notebook with outputs saved. If we run your notebook from scratch, it should produce an output (almost) identical to your PDF. You can edit your notebook however you want (on Colab, on your local machine, somewhere else); just upload the latest version of it to GitHub.\n",
    "\n",
    "Your GitHub account must submit contain all three of these, or you will lose points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UeBKTBUtABYT",
    "outputId": "738418fc-1fdd-4066-d85f-b966fe10856d"
   },
   "outputs": [],
   "source": [
    "# Choose basedir as either local or hosted directory\n",
    "import os\n",
    "if \"COLAB_BACKEND_VERSION\" in os.environ:\n",
    "    base_dir = \"/content\"\n",
    "else:\n",
    "    base_dir = os.getcwd()\n",
    "\n",
    "# get helper code from the course repository\n",
    "# install common packages used for deep learning\n",
    "%cd $base_dir\n",
    "!git clone https://github.com/cs449f23/lectures.git lectures/\n",
    "%cd $base_dir/lectures/\n",
    "!git pull -q origin main\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iBQ8tM5tAEek"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from pathlib import Path\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wnh0dTdNijSR"
   },
   "source": [
    "----\n",
    "## <a name=\"1\">Question 1: A Simple Sequence Task (6 points total)</a>\n",
    "\n",
    "In this question, we'll explore the differences between standard recurrent neural networks (RNNs) and long short-term memory networks (LSTMs) using a simple addition task akin to the one used in the [original LSTM paper](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R67wMijzivK_"
   },
   "source": [
    "### __1.1__ RNN vs LSTM (1 point)\n",
    "\n",
    "__Explain the argument for why we would expect an LSTM to outperform a standard RNN on many problems that involve processing long sequences.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQe8FInditrE"
   },
   "source": [
    "<center>\n",
    "<div style=\"color:red;background-color:#e5e5e5\">(YOUR ANSWER HERE)</div>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __1.2__ LSTM vs Transformer (1 point)\n",
    "\n",
    "__Explain the argument for why we would expect a Transformer model to outperform a LSTM on many problems that involve processing long sequences.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQe8FInditrE"
   },
   "source": [
    "<center>\n",
    "<div style=\"color:red;background-color:#e5e5e5\">(YOUR ANSWER HERE)</div>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qa5rYDb0jVal"
   },
   "source": [
    "The following utility code is provided to help with the question. Note that the addition task is formulated as a _classification_ rather than _regression_ problem: because the operands are digits, their sum must be a positive integer in [0, 18], allowing us to output a vector of \"class scores\" over these potential values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5O7PDX1Pi-md",
    "outputId": "cdc38bfc-bffc-4be7-be10-c404cd188efc"
   },
   "outputs": [],
   "source": [
    "def MakeItHot(data, num_tokens):\n",
    "    \"\"\" Make the one hot encoding of the input.\"\"\"\n",
    "    i_size, j_size = data.shape\n",
    "    one_hot_data = torch.zeros(i_size, j_size, num_tokens)\n",
    "    for i in range(0,i_size):\n",
    "        for j in range(0,j_size):\n",
    "            one_hot_data[i,j,data[i,j]] = 1\n",
    "\n",
    "    return one_hot_data\n",
    "  \n",
    "def KeepItCool(data):\n",
    "    \"\"\"Just use the data as-is...but with a new dimension added\"\"\"\n",
    "    return torch.unsqueeze(data,2)\n",
    "\n",
    "\n",
    "class Add2DigitsDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Add2DigitsDataset\"\"\"\n",
    "\n",
    "    def __init__(self, num_examples=3, seq_length=10):\n",
    "        \"\"\"Create a set of examples, where each example is a sequence of single \n",
    "        positive digits (0 through 9) interspersed with 1 instance of a negative\n",
    "        one. The game is to sum the final digit in the sequence with the \n",
    "        digit after the -1. The correct label y for the sequence x is the sum of \n",
    "        these two digits. Here are 3 examples. \n",
    "\n",
    "        x = [1, -1, 3, 7, 8, 9]\n",
    "        y = 12\n",
    "        \n",
    "        x = [9, 1, 2, 4, -1, 7, 2, 3, 1, 0]\n",
    "        y = 7\n",
    "        \n",
    "        x = [9, -1, 9, 2 , 1, 3, 0, 5, 6, 4, 7, 8, 5, 1]\n",
    "        y = 10\n",
    "\n",
    "        \n",
    "        PARAMETERS\n",
    "        ----------\n",
    "        num_examples    A non-negative integer determining how much data we'll generate\n",
    "        seq_length      A non-negative integer saying how long the sequences all are.\n",
    " \n",
    "        EXAMPLE USE\n",
    "        -----------\n",
    "        dataset = Add2DigitsDataset(num_examples = 100, seq_length = 9)\n",
    "        data, one_hot_data, labels  = dataset[:]\n",
    "        print(f'Train instances: one_hot_data (shape {one_hot_data.shape}), labels (shape {labels.shape})')\n",
    "        loader =  torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "        \"\"\"\n",
    "\n",
    "        assert seq_length >= 3, \"Seq_length must be a minimum of 3\"\n",
    "        self.seq_length = seq_length\n",
    "        self.num_tokens = 11\n",
    "\n",
    "        # make random sequences of integers of the right length\n",
    "        data = torch.randint(10, (num_examples, seq_length))\n",
    "        # the labels will go here...\n",
    "        label = torch.ones(num_examples,1)\n",
    "\n",
    "        # Now insert our special tokens \n",
    "        for x in range(0, num_examples):\n",
    "            # figure out where to place our special token \n",
    "            a_idx = torch.randint(0, seq_length-2, (1,1)).squeeze()\n",
    "\n",
    "            # insert the special_tokens \n",
    "            data[x,[a_idx]] = -1\n",
    "\n",
    "            # create the label by summing the digit after the special -1 token\n",
    "            # with the final digit in the sequence\n",
    "            label[x] = data[x,a_idx+1]+data[x,-1]\n",
    "        \n",
    "        # OK. Store data for later.\n",
    "        self.data = KeepItCool(data)\n",
    "        self.one_hot_data = MakeItHot(data, num_tokens=self.num_tokens)\n",
    "        self.label = label.squeeze().long()\n",
    "        self.lengths = self.seq_length * torch.ones_like(self.label)\n",
    "        self.lengths = self.lengths.squeeze().long().tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.one_hot_data[idx], self.label[idx] #, self.lengths[idx]\n",
    "\n",
    "# Make sure our dataset looks correct by looking at a few examples...\n",
    "dataset = Add2DigitsDataset(num_examples=100,seq_length=5)\n",
    "data, one_hot_data, labels = dataset[:3]\n",
    "# inputs have shape (B, MAX_L, V) where MAX_L is largest length in batch\n",
    "print(\"Data  \" + \"==\" * 10)\n",
    "print(f\"Shape: {data.shape}\")\n",
    "print(data)\n",
    "print()\n",
    "\n",
    "print(\"One-hot Encoding  \" + \"==\" * 10)\n",
    "print(f\"Shape: {one_hot_data.shape}\")\n",
    "print(one_hot_data)\n",
    "print()\n",
    "print(\"Labels  \" + \"==\" * 10)\n",
    "print(f\"Shape: {labels.shape}\")\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gUNiLM4-janW",
    "outputId": "26bd0cfa-0237-4162-ca9b-3ecf733c331d"
   },
   "outputs": [],
   "source": [
    "class MyRNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A simple RNN that that lets you select architectural features when creating\n",
    "    a new instance\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "               input_size: int, \n",
    "               hidden_size: int, \n",
    "               output_size: int,\n",
    "               num_layers: int = 1,\n",
    "               use_LSTM: bool = True):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if use_LSTM:  \n",
    "            self.rnn = torch.nn.LSTM(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True)\n",
    "        else: \n",
    "            self.rnn = torch.nn.RNN(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True)\n",
    "\n",
    "        self.out_proj = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "\n",
    "        # require batched inputs: (B, MAX_L, V)\n",
    "        # Here b = the batch size, l = sequence length , v = input token size (i.e. \n",
    "        # the number of positiions in a one-hot-encoding array)\n",
    "        assert x.ndim == 3\n",
    "        b, l, v = x.shape\n",
    "\n",
    "        # built-in PyTorch layer handles loop along sequence dimension,\n",
    "        # including passing hidden state back each time step. It also \n",
    "        # handles creating a new initial state for each batch!\n",
    "        output, hidden = self.rnn(x)\n",
    "\n",
    "        # for each item in batch, take final output state \n",
    "        output = output[:,-1,:]\n",
    "\n",
    "        # apply final linear layer to get predictions\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# declare the model and try out th untrained model on a sequence\n",
    "model = MyRNN(\n",
    "    input_size=11,\n",
    "    hidden_size= 128,\n",
    "    output_size=19, # allow outputs in range [0, 18]\n",
    "    num_layers=2,  \n",
    "    use_LSTM=False)\n",
    "\n",
    "# you should use `num_token` = 11: ten tokens for the digits, plus the special -1 token\n",
    "input =  MakeItHot(torch.tensor([[1,2,3,1,5,6,-1,9,3,4,5,0]]), num_tokens=11)\n",
    "prediction = model(input)\n",
    "print('My prediction is ', torch.argmax(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCD0bas5jw1m"
   },
   "outputs": [],
   "source": [
    "### Your experiment code goes here\n",
    "### =============================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrjyWdqdi0K_"
   },
   "source": [
    "### __1.3__ Empirical Comparison (2 points)\n",
    "\n",
    "Use the code provided above to compare the performance of a standard RNN to an LSTM on the task defined by the `Add2DigitsDataset`. Do this for three different lengths of sequence (5, 10, and 30). For all experiments, use a hidden size of 128 and 2 layers. Keep your network architecture the same, except for the substitution of the LSTM for an RNN. Provide the following graphs __for each sequence length__:\n",
    "\n",
    "  1. __Loss as a function of the number of training steps__  \n",
    "  2. __Training accuracy & validation accuracy as a function of the number of training steps.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1syTcxn_jK_N"
   },
   "source": [
    "<center>\n",
    "<div style=\"color:red;background-color:#e5e5e5\">(YOUR LOSS PLOTS HERE)</div>\n",
    "    .\n",
    "<div style=\"color:red;background-color:#e5e5e5\">(YOUR ACCURACY PLOTS HERE)</div>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1goyJHHnj2_G"
   },
   "source": [
    "### __1.4__ Analysis (2 point)\n",
    "\n",
    "Provide your analysis of the above experiment from Question 1.3. Make sure to mention:\n",
    "- Was the hypothesis from question 1.1 supported by your data? Why or why not?\n",
    "- What might be the limitations of your experiment? How would you improve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQe8FInditrE"
   },
   "source": [
    "<center>\n",
    "<div style=\"color:red;background-color:#e5e5e5\">(YOUR ANSWER HERE)</div>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR0kihadjt5C"
   },
   "source": [
    "----\n",
    "## <a name=\"2\">Question 2: Sequence-to-Sequence Translation (10 points total)</a>\n",
    "\n",
    "Sequence-to-sequence tasks such as language translation require a model capable of converting variable-length inputs to variable-length outputs, with no guarantee that each position in the input will map directly to a position in the output. This can be thought of as a \"many-to-many\" task, as illustrated by the second figure from the right in Andrej Karpathy's diagram:\n",
    "\n",
    "<br/>\n",
    "<center>\n",
    "<img width=\"600px\" src=\"https://raw.githubusercontent.com/cs449s23/lectures/main/static/hw4_rnns.jpeg\" />\n",
    "</center>\n",
    "<br/>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "Image source: \"The Unreasonable Effectiveness of Recurrent Neural Networks\" (Karpathy)\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "For example, consider the following English-to-Spanish translation:\n",
    "\n",
    "`I like chocolate` --> `Me gusta el chocolate`\n",
    "\n",
    "The input sequence contains three words, while the output sequence contains four. Moreover, the individual words and grammatical structure of the sentences do not cleanly correspond, as `chocolate` is preceded by the definite article `el` and the subject switches from the speaker to the chocolate. It's easy to see why sequence-to-sequence translation can be a challenging task!\n",
    "\n",
    "To overcome these difficulties, [Sutskever et al.](https://arxiv.org/pdf/1409.3215.pdf) proposed to use recurrent __encoder__ and __decoder__ networks. First, special __start and stop tokens__ are added to the ends of all sentences. Then, the encoder RNN loops over an input sequence, and its final hidden state is taken as a representation of the entire input \"context.\" This context is passed to the decoder RNN, which generates one word at a time __autoregressively__ (taking its own past predictions as inputs) until it produces a stop token. This allows for arbitrary input and output lengths, as (a) the only representation of the input that the decoder sees is a single context vector, and (b) the decoder is free to generate for as long as it wants to before emitting a stop token.\n",
    "\n",
    "<br/>\n",
    "<center>\n",
    "<img width=\"600px\" src=\"https://raw.githubusercontent.com/cs449s23/lectures/main/static/hw4_sutskever.png\" />\n",
    "</center>\n",
    "<br/>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "Image source: \"Sequence to Sequence Learning with Neural Networks\" (Sutskever et al. 2014). \"EOS\" is the end-of-sentence stop token.\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "In this question, we'll explore how to use the encoder-decoder architecture to perform Turkish-to-English translation. This will require a bit of setup, which is documented in the next section. To simplify the process and avoid messing with your local Python environment, it's recommended that you use [Google Colab](https://colab.research.google.com/).\n",
    "\n",
    "Part of this notebook was adapted from a tutorial by Ben Trevett."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "te2cHR2GAGvo",
    "outputId": "6d09698b-d736-4efb-87dd-e6db0599cc0e"
   },
   "outputs": [],
   "source": [
    "# Download 1000 training examples and 100 test examples\n",
    "# for a Turkish <-> English translation dataset\n",
    "\n",
    "train_dataset = load_dataset(\"wmt16\", \"tr-en\", split=\"train[:1000]\")\n",
    "test_dataset = load_dataset(\"wmt16\", \"tr-en\", split=\"test[:100]\")\n",
    "\n",
    "tr_tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
    "print(f\"Turkish vocab size: {tr_tokenizer.vocab_size}\")\n",
    "en_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(f\"English vocab size: {en_tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l_NA4wcDFUaQ",
    "outputId": "040e0a97-6679-4ffb-a083-16cb973a1429"
   },
   "outputs": [],
   "source": [
    "# Show an example and the tokenization of the text\n",
    "\n",
    "example = next(iter(train_dataset))[\"translation\"]\n",
    "print(\"# Turkish\")\n",
    "print(example[\"tr\"])\n",
    "print(tr_tokenizer(example[\"tr\"])[\"input_ids\"])\n",
    "print(tr_tokenizer.decode(tr_tokenizer(example[\"tr\"])[\"input_ids\"]))\n",
    "\n",
    "print(\"\\n# English\")\n",
    "print(example[\"en\"])\n",
    "print(en_tokenizer(example[\"en\"])[\"input_ids\"])\n",
    "print(en_tokenizer.decode(en_tokenizer(example[\"en\"])[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-oB4fnw9NhZ7",
    "outputId": "cab26de4-c14f-4412-f5a8-c7b05a72f06e"
   },
   "outputs": [],
   "source": [
    "# Preprocess and batch our datasets\n",
    "# Here, we are fixing sequences to be no longer than 20 tokens,\n",
    "#   which is a substantial limitation. However, it makes things faster.\n",
    "MAX_LENGTH = 20\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def tokenize(example):\n",
    "    pair = example[\"translation\"]\n",
    "    tr_tokens = tr_tokenizer(pair[\"tr\"])[\"input_ids\"][:MAX_LENGTH]\n",
    "    en_tokens = en_tokenizer(pair[\"en\"])[\"input_ids\"][:MAX_LENGTH]\n",
    "    retval = {\"tr_tokens\": tr_tokens, \"en_tokens\": en_tokens}\n",
    "\n",
    "    return retval\n",
    "\n",
    "def collate(batch):\n",
    "    tr_tokens = [torch.Tensor(x[\"tr_tokens\"]).long().to(device) for x in batch]\n",
    "    tr_batch = torch.nn.utils.rnn.pad_sequence(tr_tokens, batch_first=True,)\n",
    "\n",
    "    en_tokens = [torch.Tensor(x[\"en_tokens\"]).long().to(device) for x in batch]\n",
    "    en_batch = torch.nn.utils.rnn.pad_sequence(en_tokens, batch_first=True)\n",
    "\n",
    "    return (tr_batch, en_batch)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize)\n",
    "batched_train = DataLoader(tokenized_train, batch_size=64,\n",
    "                           drop_last=False, collate_fn=collate)\n",
    "tokenized_test = test_dataset.map(tokenize)\n",
    "batched_test = DataLoader(tokenized_test, batch_size=64,\n",
    "                           drop_last=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6DMIJ4-PWIL",
    "outputId": "37d77b71-b288-4fcf-af07-f8f0037fe8fc"
   },
   "outputs": [],
   "source": [
    "# Print out some (padded, batched) examples\n",
    "for example in batched_train:\n",
    "    src, trg = example\n",
    "    print(src[:5, :])\n",
    "    print(trg[:5, :])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otKAwY5tk5eF"
   },
   "source": [
    "# __2.1__ RNN Translation (3 points)\n",
    "\n",
    "Finally, we can train an encoder-decoder model to perform sequence-to-sequence translation.\n",
    "\n",
    "Modify the definitions in the cell below as instructed by the accompanying comments. Then, run the following cell to train and evaluate your encoder-decoder network for Turkish-to-English translation. Finally, include a plot of your training and validation losses by epoch where instructed below.\n",
    "\n",
    "__Hints__:\n",
    "1. If your implementation is \"correct,\" you should see your loss decrease as your model trains. You can also check your implementation by running the example translation provided after the training code.\n",
    "\n",
    "1. When you're initially trying to get your code to run, don't worry about whether the model outputs \"the\" over and over again or has other silly behaviors.\n",
    "\n",
    "1. We are limiting the sentence length to `MAX_LENGTH=20` tokens (not 20 words), so the sentences you see in your output may be shorter than you expect.\n",
    "\n",
    "1. __It may take ~500 epochs for the model to fully learn the training data!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0Woiwb-rY3r"
   },
   "outputs": [],
   "source": [
    "class RNNEncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 input_size: int, \n",
    "                 embedding_size: int, \n",
    "                 hidden_size: int, \n",
    "                 depth: int,\n",
    "                 dropout: float):\n",
    "      \n",
    "        super().__init__()\n",
    "                \n",
    "        self.hidden_size = hidden_size\n",
    "        self.depth = depth\n",
    "        \n",
    "        # embedding layer: similar to one-hot encoding, we will map\n",
    "        # integer indices to vectors\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n",
    "        \n",
    "        ###########################################################\n",
    "        # YOUR CODE STARTS HERE (1/5)\n",
    "        ###########################################################\n",
    "\n",
    "        ###########################################################\n",
    "        # YOUR CODE ENDS HERE (1/5)\n",
    "        ###########################################################\n",
    "        \n",
    "        # a dropout layer can improve generalization\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder RNN.\n",
    "        Input x: source sentence (e.g., in Turkish)\n",
    "        Returns: final hidden state of the RNN\n",
    "        \"\"\"\n",
    "        \n",
    "        assert x.ndim == 2\n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        # convert integer indices to vector representations\n",
    "        embedded = self.dropout(self.embedding(x))  # (batch_size, seq_len, embedding_size)\n",
    "\n",
    "        ###########################################################\n",
    "        # YOUR CODE STARTS HERE (2/5)\n",
    "        ###########################################################\n",
    "\n",
    "        ###########################################################\n",
    "        # YOUR CODE ENDS HERE (2/5)\n",
    "        ###########################################################\n",
    "\n",
    "class RNNDecoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 output_size: int, \n",
    "                 embedding_size: int, \n",
    "                 hidden_size: int, \n",
    "                 depth: int,\n",
    "                 dropout: float):\n",
    "      \n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.depth = depth\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # embedding layer: similar to one-hot encoding, we will map\n",
    "        # integer indices to vectors\n",
    "        self.embedding = torch.nn.Embedding(output_size, embedding_size)\n",
    "\n",
    "        ###########################################################\n",
    "        # YOUR CODE STARTS HERE (3/5)\n",
    "        ###########################################################\n",
    "\n",
    "        ###########################################################\n",
    "        # YOUR CODE ENDS HERE (3/5)\n",
    "        ###########################################################\n",
    "        \n",
    "        # a dropout layer can improve generalization\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, hidden: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder RNN.\n",
    "        Input x: target sentence input (e.g., in English)\n",
    "        Returns: prediction for next token, and final hidden state of RNN\n",
    "        \"\"\"\n",
    "        \n",
    "        # take a single time-step of input and a single hidden state\n",
    "        if x.ndim == 1:\n",
    "            x = x.unsqueeze(1)\n",
    "        batch_size, _ = x.shape\n",
    "        \n",
    "        # convert integer indices to vector representations\n",
    "        embedded = self.dropout(self.embedding(x))  # (batch_size, seq, embedding_size)\n",
    "\n",
    "        ###########################################################\n",
    "        # YOUR CODE STARTS HERE (4/5)\n",
    "        ###########################################################\n",
    "\n",
    "        ###########################################################\n",
    "        # YOUR CODE ENDS HERE (4/5)\n",
    "        ###########################################################\n",
    "\n",
    "\n",
    "class RNNTranslator(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 encoder: torch.nn.Module, \n",
    "                 decoder: torch.nn.Module, \n",
    "                 device):\n",
    "      \n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hidden_size == decoder.hidden_size, \\\n",
    "        f\"Mismatch: `encoder.hidden_size` = {encoder.hidden_size}, `decoder.hidden_size` = {decoder.hidden_size}\"\n",
    "        \n",
    "        assert encoder.depth == decoder.depth, \\\n",
    "        f\"Mismatch: `encoder.depth` = {encoder.depth}, `decoder.depth` = {decoder.depth}\"\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor, teacher_forcing_ratio: float = 0.5):\n",
    "        \"\"\"\n",
    "        Translate a sentence from source (e.g., Turkish) to target (e.g., English)\n",
    "        \"\"\"\n",
    " \n",
    "        # inputs should have shape (batch_size, in_seq_len)\n",
    "        assert x.ndim == 2\n",
    "        batch_size, in_seq_len = x.shape\n",
    "\n",
    "        # targets should have shape (batch size, out_seq_len,)\n",
    "        assert y.ndim == 2\n",
    "        _, out_seq_len = y.shape\n",
    "        out_vocab_size = self.decoder.output_size\n",
    "        \n",
    "        # prepare to store decoder outputs. We only need to allocate \n",
    "        # the \"true\" number of time steps during training, but for inference\n",
    "        # we can simply pass in an empty `y` of any length; this will serve as\n",
    "        # a maximum length constraint on the output translation (which may end\n",
    "        # sooner if an <EOS> token is predicted).\n",
    "        outputs = []\n",
    "        \n",
    "        ###########################################################\n",
    "        # YOUR CODE STARTS HERE (5/5)\n",
    "        ###########################################################\n",
    "\n",
    "        ###########################################################\n",
    "        # YOUR CODE ENDS HERE (5/5)\n",
    "        ###########################################################\n",
    "\n",
    "        # first input to the decoder is the <SOS> start token\n",
    "        input = y[:,:1]\n",
    "\n",
    "        for t in range(1, out_seq_len):\n",
    "\n",
    "            # given input for current time step (either previous decoder \n",
    "            # prediction or <SOS> token) and previous hidden state, compute\n",
    "            # new output and hidden states\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "\n",
    "            # write predicted word probabilities to output buffer\n",
    "            outputs.append(output)\n",
    "            \n",
    "            # during training, we may randomly decide whether to use\n",
    "            # the decoder's previous predictions or the ground-truth\n",
    "            # tokens for the previous time step. When we do the latter,\n",
    "            # it is called \"teacher-forcing.\" We will randomly apply \n",
    "            # teacher-forcing during training only.\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            # take highest-scoring token from decoder's previous prediction\n",
    "            top1 = output.argmax(1).unsqueeze(1)\n",
    "            \n",
    "            # optionally apply teacher-forcing\n",
    "            input = y[:, t] if teacher_force else top1\n",
    "        \n",
    "        outputs = torch.stack(outputs, axis=1).to(self.device)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zb6CxsyOghai"
   },
   "source": [
    "# 2.2 LSTM Translation (2 points)\n",
    "\n",
    "The following code (almost) implements an LSTM variation on the RNN translator you implemented above.\n",
    "Until you have your RNNTranslator class (above) running with the experimental code (below), you should skip this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05uC5R2waNnC"
   },
   "outputs": [],
   "source": [
    "class LSTMEncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size: int, \n",
    "                 embedding_size: int, \n",
    "                 hidden_size: int, \n",
    "                 depth: int,\n",
    "                 dropout: float):\n",
    "      \n",
    "        super().__init__()\n",
    "                \n",
    "        self.hidden_size = hidden_size\n",
    "        self.depth = depth\n",
    "        \n",
    "        # embedding layer: similar to one-hot encoding, we will map\n",
    "        # integer indices to vectors\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n",
    "        \n",
    "        ###########################################################\n",
    "        # YOUR CODE STARTS HERE (1/5)\n",
    "        ###########################################################\n",
    "        \n",
    "        ###########################################################\n",
    "        # YOUR CODE ENDS HERE (1/5)\n",
    "        ###########################################################\n",
    "        \n",
    "        # a dropout layer can improve generalization\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder RNN.\n",
    "        Input x: source sentence (e.g., in Turkish)\n",
    "        Returns: final hidden state of the RNN\n",
    "        \"\"\"\n",
    "\n",
    "        assert x.ndim == 2\n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        # convert integer indices to vector representations\n",
    "        embedded = self.dropout(self.embedding(x))  # (batch_size, seq_len, embedding_size)\n",
    "        \n",
    "        ###########################################################\n",
    "        # YOUR CODE STARTS HERE (2/5)\n",
    "        ###########################################################\n",
    "\n",
    "        ###########################################################\n",
    "        # YOUR CODE ENDS HERE (2/5)\n",
    "        ###########################################################\n",
    "\n",
    "\n",
    "class LSTMDecoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 output_size: int, \n",
    "                 embedding_size: int, \n",
    "                 hidden_size: int, \n",
    "                 depth: int,\n",
    "                 dropout: float,):\n",
    "      \n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.depth = depth\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # embedding layer: similar to one-hot encoding, we will map\n",
    "        # integer indices to vectors\n",
    "        self.embedding = torch.nn.Embedding(output_size, embedding_size)\n",
    "\n",
    "        ###########################################################\n",
    "        # YOUR CODE STARTS HERE (3/5)\n",
    "        ###########################################################\n",
    "\n",
    "        ###########################################################\n",
    "        # YOUR CODE ENDS HERE (3/5)\n",
    "        ###########################################################\n",
    "        \n",
    "        # a dropout layer can improve generalization\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, hidden: torch.Tensor, cell: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder RNN.\n",
    "        Input x: target sentence input (e.g., in English)\n",
    "        Returns: prediction for next token, and final hidden state of RNN\n",
    "        \"\"\"\n",
    "\n",
    "        # take a single time-step of input, a single hidden state, and a single cell state\n",
    "        if x.ndim == 1:\n",
    "            x = x.unsqueeze(1)\n",
    "        batch_size, _ = x.shape\n",
    "        \n",
    "        # convert integer indices to vector representations\n",
    "        embedded = self.dropout(self.embedding(x))  # (batch_size, 1, embedding_size)\n",
    "\n",
    "        ###########################################################\n",
    "        # YOUR CODE STARTS HERE (4/5)\n",
    "        ###########################################################\n",
    "\n",
    "        ###########################################################\n",
    "        # YOUR CODE ENDS HERE (4/5)\n",
    "        ###########################################################\n",
    "\n",
    "\n",
    "class LSTMTranslator(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 encoder: torch.nn.Module,\n",
    "                 decoder: torch.nn.Module,\n",
    "                 device: str):\n",
    "      \n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hidden_size == decoder.hidden_size, \\\n",
    "        f\"Mismatch: `encoder.hidden_size` = {encoder.hidden_size}, `decoder.hidden_size` = {decoder.hidden_size}\"\n",
    "        \n",
    "        assert encoder.depth == decoder.depth, \\\n",
    "        f\"Mismatch: `encoder.depth` = {encoder.depth}, `decoder.depth` = {decoder.depth}\"\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor, teacher_forcing_ratio: float = 0.5):\n",
    "        \"\"\"\n",
    "        Translate a sentence from source (e.g., Turkish) to target (e.g., English)\n",
    "        \"\"\"\n",
    "        # inputs should have shape (batch_size, in_seq_len)\n",
    "        assert x.ndim == 2\n",
    "        batch_size, in_seq_len = x.shape\n",
    "\n",
    "        # targets should have shape (batch size, out_seq_len,)\n",
    "        assert y.ndim == 2\n",
    "        _, out_seq_len = y.shape\n",
    "        out_vocab_size = self.decoder.output_size\n",
    "        \n",
    "        # prepare to store decoder outputs. We only need to allocate \n",
    "        # the \"true\" number of time steps during training, but for inference\n",
    "        # we can simply pass in an empty `y` of any length; this will serve as\n",
    "        # a maximum length constraint on the output translation (which may end\n",
    "        # sooner if an <EOS> token is predicted).\n",
    "        outputs = []\n",
    "        \n",
    "        ###########################################################\n",
    "        # YOUR CODE STARTS HERE (5/5)\n",
    "        ###########################################################\n",
    "\n",
    "        ###########################################################\n",
    "        # YOUR CODE ENDS HERE (5/5)\n",
    "        ###########################################################\n",
    "\n",
    "        # first input to the decoder is the <SOS> start token\n",
    "        input = y[:,:1]\n",
    "\n",
    "        for t in range(1, out_seq_len):\n",
    "\n",
    "            # given input for current time step (either previous decoder \n",
    "            # prediction or <SOS> token) and previous hidden state and\n",
    "            # cell state, compute new output, hidden, and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "\n",
    "            # save predicted word probabilities\n",
    "            outputs.append(output)\n",
    "            \n",
    "            # during training, we may randomly decide whether to use\n",
    "            # the decoder's previous predictions or the ground-truth\n",
    "            # tokens for the previous time step. When we do the latter,\n",
    "            # it is called \"teacher-forcing.\" We will randomly apply \n",
    "            # teacher-forcing during training only.\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            # take highest-scoring token from decoder's previous prediction\n",
    "            top1 = output.argmax(1).unsqueeze(1)\n",
    "            \n",
    "            # optionally apply teacher-forcing\n",
    "            input = y[:, t] if teacher_force else top1\n",
    "        \n",
    "        outputs = torch.stack(outputs, axis=1).to(self.device)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zzup7jSTg1h8"
   },
   "source": [
    "# 2.3 Translation experiments (5 points total)\n",
    "\n",
    "For either the RNN or the LSTM, you will use the same experimental code below. The hyperparameter values we've given you are ones that worked for us, but you are welcome to change them. You shouldn't need to change any training code below the line that says \"Don't change code below this line.\"\n",
    "\n",
    "As you run experiments, keep track of what hyperparameters you're using and how the model trains. Are you able to learn anything? Does your model overfit? You'll answer these below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 721
    },
    "id": "SROEMTPNvGWs",
    "outputId": "903a3d09-9d74-4718-a472-63481ea4a114"
   },
   "outputs": [],
   "source": [
    "# Training hyperparameters; you are welcome to change these.\n",
    "# In my experiments, it can take ~500 epochs over the dataset\n",
    "#   of 1000 training examples before it perfectly\n",
    "#   translates the training examples.\n",
    "\n",
    "ENC_EMB_DIM = 1\n",
    "DEC_EMB_DIM = 1\n",
    "HID_DIM = 1\n",
    "N_LAYERS = 1\n",
    "ENC_DROPOUT = 0.0\n",
    "DEC_DROPOUT = 0.0\n",
    "N_EPOCHS = 1\n",
    "CLIP = 1\n",
    "LR = 1\n",
    "USE_LSTM = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "###  Don't change code below this line\n",
    "###  =================================\n",
    "\n",
    "INPUT_DIM = tr_tokenizer.vocab_size\n",
    "OUTPUT_DIM = en_tokenizer.vocab_size\n",
    "TRG_PAD_IDX = 0\n",
    "\n",
    "# initialize encoder-decoder model\n",
    "if USE_LSTM:\n",
    "    enc = LSTMEncoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT).to(device)\n",
    "    dec = LSTMDecoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT).to(device)\n",
    "    model = LSTMTranslator(enc, dec, device)\n",
    "else:\n",
    "    enc = RNNEncoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT).to(device)\n",
    "    dec = RNNDecoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT).to(device)\n",
    "    model = RNNTranslator(enc, dec, device)\n",
    "\n",
    "# initialize model weights\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        torch.nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "model.apply(init_weights)\n",
    "\n",
    "# parameter count\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "# optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "\n",
    "# track training and validation losses\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip, verbose=False):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        (src, trg) = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "\n",
    "        # Every so often, print out how well we're doing.\n",
    "        if verbose and i == 0:\n",
    "            print(en_tokenizer.decode(trg[0, 1:]))\n",
    "            en_tokens = output[0, :, :].argmax(1)\n",
    "            print(en_tokenizer.decode(en_tokens))\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.reshape(-1, output_dim)\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            (src, trg) = batch \n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "    \n",
    "start_time = time.time()\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    verbose = (epoch == 0 or N_EPOCHS <= 10\n",
    "               or (epoch + 1) % (N_EPOCHS // 10) == 0)\n",
    "\n",
    "    train_loss = train(model, batched_train, optimizer,\n",
    "                       criterion, CLIP, verbose=verbose)\n",
    "    valid_loss = evaluate(model, batched_test, criterion)\n",
    "    \n",
    "    training_loss.append(train_loss)\n",
    "    validation_loss.append(valid_loss)\n",
    "    \n",
    "    # Every so often, print some details on how we're doing.\n",
    "    if verbose:\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f}')    \n",
    "        start_time = time.time()\n",
    "\n",
    "# At the end, enerate loss plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(training_loss, label=\"training\")\n",
    "plt.plot(validation_loss, label=\"validation\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKQnWO2NmaPj"
   },
   "source": [
    "If you want to explore how sensitive the model is to small changes in the input, you can translate things into Turkish using Google Translate and then see how they are affected here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PL5PKti8Hr3t",
    "outputId": "f147e5f9-0be8-4f6c-f858-423984ce0fb7"
   },
   "outputs": [],
   "source": [
    "sentence = \"Kosova'nın özelleştirme süreci büyüteç altında\"\n",
    "\n",
    "tokens = tr_tokenizer(sentence)[\"input_ids\"]\n",
    "tokens = torch.tensor([tokens]).long().to(device)\n",
    "print(tokens)\n",
    "dummy_output = torch.ones([1, MAX_LENGTH]).long().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(tokens, dummy_output, 0).cpu()\n",
    "    en_tokens = output[0, :, :].argmax(1)\n",
    "    print(en_tokenizer.decode(en_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tshw6P_umn3J"
   },
   "source": [
    "## 2.3.1 Experimental Results (5 points)\n",
    "\n",
    "Write at least five sentences about what you noticed during your experiments. Include at least two plots of successful training curves over time. Make sure to mention:\n",
    "\n",
    "- What hyperparameters had the most effect on your model's ability to reduce the *training* loss?\n",
    "- What about for the *validation* loss?\n",
    "- What was necessary to prevent underfitting?\n",
    "- What were you able to do to prevent overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQe8FInditrE"
   },
   "source": [
    "<center>\n",
    "<div style=\"color:red;background-color:#e5e5e5\">(YOUR ANSWER HERE)</div>\n",
    ".\n",
    "<div style=\"color:red;background-color:#e5e5e5\">(Don't forget your plots!)</div>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gudrVbRvtIx3"
   },
   "source": [
    "# 3. Conceptual Questions (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UAVLCcmnxf7"
   },
   "source": [
    "### 3.1 RNN versus Fully-connected (1.5 points)\n",
    "\n",
    "Let's compare the number of parameters in an RNN versus in a fully-connected MLP.\n",
    "\n",
    "Suppose we want a model that can do *binary classification* on sequences, instead of translation. We're given a fixed embedding matrix that maps from our vocabulary of 10,000 tokens to a 100-dimensional vector. Don't count the embedding matrix's parameters in your counts below.\n",
    "\n",
    "Consider two different models. A visualization of each is provided after the description. In the visualizations, the arrows from the words (e.g., \"cat\") to the embeddings (e.g., $e_2$) are embedding lookups; remember that you shouldn't count the embedding matrix's parameters in your counts. All other arrows represent some sort of matrix multiplication (which you can assume includes a bias term). You can assume that the final output vector has two dimensions (for the probability of each class).\n",
    "\n",
    "1. A MLP with two hidden fully-connected layers with 256 hidden nodes each followed by a final fully-connected layer for binary classification.\n",
    "<center>\n",
    "<img width=\"500px\" src=\"https://raw.githubusercontent.com/cs449s23/lectures/cfc1b469e2d26e9811ff87d56682d5b65dfb941a/static/hw4_mlp.png\">\n",
    "</center>\n",
    "\n",
    ".\n",
    "\n",
    "2. A torch.nn.RNN like the one you used above, with two hidden layers, a hidden size of 256, and a final fully-connected layer that outputs a binary classification.\n",
    "<center>\n",
    "<img width=\"500px\" src=\"https://raw.githubusercontent.com/cs449s23/lectures/cfc1b469e2d26e9811ff87d56682d5b65dfb941a/static/hw4_rnn.png\">\n",
    "</center>\n",
    "\n",
    "Answer the following three questions:\n",
    "\n",
    "* (a.) Let's assume that all sequences are exactly 10 tokens long. To the nearest thousand parameters, how many parameters in total do the MLP and RNN each have?\n",
    "* (b.) Suppose instead that all sequences are exactly 40 tokens long. Now, to the nearest thousand parameters, how many parameters in total does each model have?\n",
    "* (c.) What does this say about when you might prefer an RNN or an MLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQe8FInditrE"
   },
   "source": [
    "<center>\n",
    "<div style=\"color:red;background-color:#e5e5e5\">(YOUR ANSWER HERE)</div>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxxSdvVFq4fx"
   },
   "source": [
    "### 3.2 RNNs versus Transformers (1.5 points)\n",
    "\n",
    "Suppose you wanted to implement a `TransformerTranslator` model like the ones you implemented above with RNNs or LSTMs. Let's assume you wanted to maintain a similar Encoder and Decoder structure. Answer the following questions:\n",
    "\n",
    "* (a.) What would be one theoretical advantage of using Transformer models over LSTMs?\n",
    "* (b.) What would be one theoretical advantage of using LSTMs over Transformers?\n",
    "* (c.) Consider the input tensors and their shapes that are passed into `self.encoder()` and `self.decoder()` inside of the `LSTMTranslator` class. If your encoder and decoder were both Transformer models, what inputs would they take? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQe8FInditrE"
   },
   "source": [
    "<center>\n",
    "<div style=\"color:red;background-color:#e5e5e5\">(YOUR ANSWER HERE)</div>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgTUWe3cwaUM"
   },
   "source": [
    "### 3.3 Teacher forcing (1 point)\n",
    "\n",
    "In the code we provide, there is a hyperparameter for \"teacher-forcing\" which you can read more about here: https://en.wikipedia.org/wiki/Teacher_forcing\n",
    "\n",
    "Then, answer the following questions:\n",
    "\n",
    "* (a.) Why is it important that we set the probability of teacher forcing to 0% at test time?\n",
    "* (b.) Under what circumstances might you want to have a higher probability during training time? A lower probability? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQe8FInditrE"
   },
   "source": [
    "<center>\n",
    "<div style=\"color:red;background-color:#e5e5e5\">(YOUR ANSWER HERE)</div>\n",
    "</center>\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
